{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cebebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2TokenizerFast, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b724a5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmacosta\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "186a8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_SAVEDIR = Path('/home/macosta/ttmp/primus-data/visual-tokens-7/')\n",
    "LM_MODEL_SAVEDIR = Path('/home/macosta/ttmp/primus-models/gpt2-lm-visual-tokens-7/')\n",
    "Path(LM_MODEL_SAVEDIR).mkdir(exist_ok=True)\n",
    "TXT_FILES = Path('/home/macosta/ttmp/primus-data/leipzig-filtered/leipzig-delim-7/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2343163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.cuda.device at 0x7ff7d3072dd0>, <torch.cuda.device at 0x7ff7d3072f90>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.cuda.device(i) for i in range(torch.cuda.device_count())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91d546d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available:  True\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "print('Cuda available: ', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93251a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab876509",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file /home/macosta/ttmp/primus-data/visual-tokens-7/config.json not found\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 30000\n",
    "MAX_LEN = 64\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(TOKENIZER_SAVEDIR, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0f5d68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0000000000000000000000000000000000000000000000000011100000000001111111100000000000000011100000000110000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000000111111111110000000000000011100000000110000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000001111111111111000000000000011100000000110000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000011110001111111100000000000011100000001100000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000111100011111111100000000000011100000001100000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100001111000011111111100000000000011100000011000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100001110000111111111110000000000011100000011000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100001110000111111111110000000000011100000111000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100011110000111111111100000000000011100001110000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100011110000111111111100000000000011100001110000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100011110000011111111000000000000011100011100000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100011110000011111110000000000000011100111100000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100011110000000111100000000000000011100111000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100011110000000011100000000000000011101111000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100011110000000011100000000000000011111110000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100011111000000011100000000000000011111100000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100001111000000011100000000000000011111000000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100001111100000011100000000000001111111000000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100001111110000011100000000000111111110000000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000111111110011100000000111111111100000000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000111111111111100111111111111111100000000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000011111111111111111111111111111100000000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000001111111111111111111111111011100000000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000000111111111111111111111100011100000000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000000011111111111111111110000011100000000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000000000011111111111111000000011100000000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000000000000011111100000000000011100000000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000000000000011100000000000000011100000000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000000000000011100000000000000011100000000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000000000000011100000000000000011100000000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000001111100011100111100000000011100000000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000001111110011101111110000000011100000000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000001111110011101111110000000011100000000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000001111110011101111110000000011100000000000000011100000000000000011100000000000000000000000000000000000000000000000000_0000000000000000000000000000000000000000000000000011100000000111100011100111100000000011100000000000000011100000000000000011100000000000000000000000000000000000000000000000000',\n",
       " 5778)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e71e6bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.unk_token = '<unk>'\n",
    "tokenizer.bos_token = '<s>'\n",
    "tokenizer.eos_token = '</s>'\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6992d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTUAL_VOCAB_SIZE = len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51440fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30001"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACTUAL_VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97806405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters: 108145920\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=ACTUAL_VOCAB_SIZE,\n",
    "    n_positions=MAX_LEN,\n",
    "    n_head=12,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config=config)\n",
    "print('Num parameters:', model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f886ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, src_files, tokenizer, max_length):\n",
    "        self.examples = []\n",
    "        for src_file in tqdm(src_files):\n",
    "            words = src_file.read_text(encoding=\"utf-8\")\n",
    "            words = words.split()\n",
    "            words = ['<s>'] + words + ['</s>']\n",
    "            for i in range(0, len(words), max_length):\n",
    "                word_string = ' '.join(words[i:i+max_length])\n",
    "                tokenized = tokenizer.encode(word_string, max_length=max_length, padding='max_length')\n",
    "                assert(len(tokenized) == max_length)\n",
    "                self.examples.append(tokenized)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83f9bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_datasets(tokenizer, max_length, fraction=1.0, test_size=0.1):\n",
    "    src_files = list(Path(TXT_FILES).glob(\"**/*\"))\n",
    "    src_files = src_files[:int(len(src_files) * fraction)]\n",
    "    split_index = int(len(src_files) * (1 - test_size))\n",
    "    train_files = src_files[:split_index]\n",
    "    test_files = src_files[split_index:]\n",
    "    train_dataset = CustomDataset(train_files, tokenizer, max_length=max_length)\n",
    "    test_dataset = CustomDataset(test_files, tokenizer, max_length=max_length)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e75d711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 276/276 [00:03<00:00, 91.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 87.20it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = create_train_test_datasets(tokenizer, MAX_LEN, fraction=1, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f4fa79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     5,    88,   925,   449,    64,   767,    67,     4,  2143,\n",
       "          178, 24970, 28282,  1030,     4,     3,     3,   669,  2972,     3,\n",
       "        29791,     4,  3293,  5492,     5,     2,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12b76cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_whitespaced(txt):\n",
    "    txt = txt.replace('_', '')\n",
    "    tokens = txt.split()\n",
    "    tokens = [t for t in tokens if t[0] != '<']\n",
    "    txt = ''.join(tokens)\n",
    "    cols = [txt[i:i+175] for i in range(0, len(txt), 175)]\n",
    "    arr = [[int(x) for x in col] for col in cols]\n",
    "    arr = (1 - np.array(arr, dtype=np.uint8)) * 255\n",
    "    Image.fromarray(arr.T).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "543b95e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=666x175 at 0x7F943AFB0B10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_whitespaced(tokenizer.decode(train_dataset.__getitem__(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c75798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbd07bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "N_EVALS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d4b513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEPS = len(train_dataset) * N_EPOCHS // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5229151",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EVAL = N_STEPS // N_EVALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8510482e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=LM_MODEL_SAVEDIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    logging_steps=STEPS_PER_EVAL,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=STEPS_PER_EVAL,\n",
    "    save_total_limit=1,\n",
    "    prediction_loss_only=False,\n",
    "    report_to=\"wandb\",\n",
    "    save_steps=5000\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e0efaf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 2537\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 800\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1440, in forward\n    return_dict=return_dict,\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 897, in forward\n    output_attentions=output_attentions,\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 432, in forward\n    feed_forward_hidden_states = self.mlp(hidden_states)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 360, in forward\n    hidden_states = self.act(hidden_states)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/activations.py\", line 42, in gelu_new\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\nRuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 23.70 GiB total capacity; 19.31 GiB already allocated; 45.56 MiB free; 19.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1750919/127849066.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m                 if (\n",
      "\u001b[0;32m~/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1972\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1973\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1440, in forward\n    return_dict=return_dict,\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 897, in forward\n    output_attentions=output_attentions,\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 432, in forward\n    feed_forward_hidden_states = self.mlp(hidden_states)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 360, in forward\n    hidden_states = self.act(hidden_states)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/activations.py\", line 42, in gelu_new\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\nRuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 23.70 GiB total capacity; 19.31 GiB already allocated; 45.56 MiB free; 19.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "ret = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f70a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(LM_MODEL_SAVEDIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

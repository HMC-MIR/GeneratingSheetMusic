{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cebebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2TokenizerFast, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b724a5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmacosta\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "186a8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_SAVEDIR = Path('/home/macosta/ttmp/primus-data/primus-mei/mei-tokenizer/')\n",
    "LM_MODEL_SAVEDIR = Path('/home/macosta/ttmp/primus-models/gpt2-lm-mei/')\n",
    "Path(LM_MODEL_SAVEDIR).mkdir(exist_ok=True)\n",
    "PRIMUS_TXT_FILES = Path('/home/macosta/ttmp/primus-data/primus-mei/mei-cleaned/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2343163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.cuda.device at 0x7ff7d3072dd0>, <torch.cuda.device at 0x7ff7d3072f90>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.cuda.device(i) for i in range(torch.cuda.device_count())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91d546d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available:  True\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "print('Cuda available: ', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93251a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab876509",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file /home/macosta/ttmp/primus-data/primus-mei/mei-tokenizer/config.json not found\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 30000\n",
    "MAX_LEN = 512\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(TOKENIZER_SAVEDIR, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e71e6bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.unk_token = '<unk>'\n",
    "tokenizer.bos_token = '<s>'\n",
    "tokenizer.eos_token = '</s>'\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6992d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTUAL_VOCAB_SIZE = len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51440fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "769"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACTUAL_VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "81c9131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quarters(token):\n",
    "    if token == 'whole':\n",
    "        return 4\n",
    "    elif token == 'half':\n",
    "        return 2\n",
    "    elif token == 'quarter':\n",
    "        return 1\n",
    "    elif token == 'eighth':\n",
    "        return 0.5\n",
    "    elif token == 'sixteenth':\n",
    "        return 0.25\n",
    "    return 0\n",
    "\n",
    "def get_sixteenths(token):\n",
    "    if token == 'whole':\n",
    "        return 16\n",
    "    elif token == 'half':\n",
    "        return 8\n",
    "    elif token == 'quarter':\n",
    "        return 4\n",
    "    elif token == 'eighth':\n",
    "        return 2\n",
    "    elif token == 'sixteenth':\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def parse_time_sig(time_sig):\n",
    "    time_sig = time_sig.split('-')[1]\n",
    "    if time_sig == 'C':\n",
    "        return 4, 4\n",
    "    elif time_sig == 'C/':\n",
    "        return 2, 2\n",
    "    elif '/' in time_sig:\n",
    "        top, bottom = time_sig.split('/')\n",
    "        return int(top), int(bottom)\n",
    "    else:\n",
    "        return 4, 4\n",
    "    \n",
    "def get_rhythmic_sequence(tokens):\n",
    "    sixteenths_per_bar = 0\n",
    "    sixteenths_left_in_bar = 0\n",
    "    last_sixteenths_duration = 0\n",
    "    rhythmic_sequence = []\n",
    "    for token in tokens:\n",
    "        if len(token) > 4 and token[:4] == 'time':\n",
    "            beats_per_bar, denom = parse_time_sig(token)\n",
    "            sixteenths_per_beat = 16 // denom\n",
    "            sixteenths_per_bar = beats_per_bar * sixteenths_per_beat\n",
    "            sixteenths_left_in_bar = sixteenths_per_bar\n",
    "            last_beat_duration = 0\n",
    "        elif token == 'barline':\n",
    "            sixteenths_left_in_bar = sixteenths_per_bar\n",
    "        elif token == 'dot':\n",
    "            sixteenths_left_in_bar -= last_sixteenths_duration * 0.5\n",
    "        elif token == 'dotdot':\n",
    "            sixteenths_left_in_bar -= last_sixteenths_duration * 0.25\n",
    "        elif token == '</s>':\n",
    "            sixteenths_per_bar = 0\n",
    "            sixteenths_left_in_bar = 0\n",
    "            last_sixteenths_duration = 0\n",
    "        else:\n",
    "            sixteenth_duration = get_sixteenths(token)\n",
    "            sixteenths_left_in_bar -= sixteenth_duration\n",
    "            if sixteenth_duration > 0:\n",
    "                last_sixteenths_duration = sixteenth_duration\n",
    "        sixteenths_left_in_bar = max(sixteenths_left_in_bar, 0)\n",
    "        rhythmic_sequence.append(int(sixteenths_left_in_bar))\n",
    "    return rhythmic_sequence\n",
    "    \n",
    "def token_to_action(token):\n",
    "    if token == 'barline' or token == '</s>' or token == '<s>':\n",
    "        return (\"RESET\", 0)\n",
    "    elif token == 'dot':\n",
    "        return (\"USE_LAST_DURATION\", 0.5)\n",
    "    elif token == 'dotdot':\n",
    "        return (\"USE_LAST_DURATION\", 0.25)\n",
    "    elif len(token) > 14 and token[:14] == 'timeSignature-':\n",
    "        return (\"SET_TIMESIG\", parse_time_sig(token))\n",
    "    else:\n",
    "        return (\"DECREMENT\", get_quarters(token))\n",
    "    \n",
    "# index_to_token = {v:k for k, v in tokenizer.vocab.items()}\n",
    "# def index_to_action(index):\n",
    "#     token = index_to_token[index]\n",
    "#     return token, token_to_action(token)\n",
    "\n",
    "# index_to_action_dict = {}\n",
    "# for index in index_to_token:\n",
    "#     index_to_action_dict[index] = index_to_action(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b4c7351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/macosta/ttmp/primus-models/index-action-dict.json', 'w') as f:\n",
    "#     json.dump(index_to_action_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97806405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYING WEIGHTS!\n",
      "GETTING EMBEDDINGS!\n",
      "Num parameters: 86039808\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=ACTUAL_VOCAB_SIZE,\n",
    "    n_positions=MAX_LEN,\n",
    "    n_head=12,\n",
    "#     index_to_action = index_to_action\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config=config)\n",
    "print('Num parameters:', model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f886ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, src_files, tokenizer, max_length):\n",
    "        self.examples = []\n",
    "        for src_file in tqdm(src_files):\n",
    "            words = src_file.read_text(encoding=\"utf-8\")\n",
    "            words = words.split()\n",
    "            words = ['<s>'] + words + ['</s>']\n",
    "            for i in range(0, len(words), max_length):\n",
    "                word_string = ' '.join(words[i:i+max_length])\n",
    "                tokenized = tokenizer.encode(word_string, max_length=max_length, padding='max_length')\n",
    "                assert(len(tokenized) == max_length)\n",
    "                self.examples.append(tokenized)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2412fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safecheck_tokens(tokens):\n",
    "    if 'thirty_second' in tokens or 'sixty_fourth' in tokens:\n",
    "        return False\n",
    "    timesig = [t for t in tokens if t[:4] == 'time'][0]\n",
    "    beats_per_bar, denom = parse_time_sig(timesig)\n",
    "    sixteenths_per_beat = 16 // denom\n",
    "    sixteenths_per_bar = beats_per_bar * sixteenths_per_beat\n",
    "    return sixteenths_per_bar <= 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "54219f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RhythmicDataset(Dataset):\n",
    "    def __init__(self, src_files, tokenizer, max_length):\n",
    "        self.examples = []\n",
    "        for src_file in tqdm(src_files):\n",
    "            words = src_file.read_text(encoding=\"utf-8\")\n",
    "            words = words.split()\n",
    "            words = ['<s>'] + words + ['</s>']\n",
    "            if not safecheck_tokens(words):\n",
    "                continue\n",
    "            rhythmic_sequence = get_rhythmic_sequence(words)\n",
    "            # Chunk in groups of max_length\n",
    "            for i in range(0, len(words), max_length):\n",
    "                word_string = ' '.join(words[i:i+max_length])\n",
    "                tokenized = tokenizer.encode(word_string, max_length=max_length, padding='max_length')\n",
    "                relevant_rhythms = rhythmic_sequence[i:i+max_length]\n",
    "                # Pad rhythm information as necessary\n",
    "                if len(relevant_rhythms) < max_length:\n",
    "                    relevant_rhythms += [0] * (max_length - len(relevant_rhythms))\n",
    "                # Bit shift rhythm and add token\n",
    "                rhythm_encoded = [(r << 10) + t for r, t in zip(relevant_rhythms, tokenized)]\n",
    "                assert(len(rhythm_encoded) == max_length)\n",
    "                self.examples.append(rhythm_encoded)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83f9bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_datasets(tokenizer, max_length, fraction=1.0, test_size=0.1):\n",
    "    src_files = list(Path(PRIMUS_TXT_FILES).glob(\"**/*.mei\"))\n",
    "    src_files = src_files[:int(len(src_files) * fraction)]\n",
    "    split_index = int(len(src_files) * (1 - test_size))\n",
    "    train_files = src_files[:split_index]\n",
    "    test_files = src_files[split_index:]\n",
    "    train_dataset = CustomDataset(train_files, tokenizer, max_length=max_length)\n",
    "    test_dataset = CustomDataset(test_files, tokenizer, max_length=max_length)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e75d711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 8328/8328 [00:03<00:00, 2118.65it/s]\n",
      "100%|███████████████████████████████████████| 439/439 [00:00<00:00, 2096.10it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = create_train_test_datasets(tokenizer, MAX_LEN, fraction=1, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f4fa79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 47,  4, 45,  4, 46,  4, 48,  4, 37, 55, 81, 57, 66,  4, 50,  4, 54,\n",
       "        52, 65, 51, 11, 12, 11, 53, 13,  5, 44,  4, 42,  4, 49,  4, 21, 20, 22,\n",
       "         4, 19, 12, 11,  4, 18, 12, 11,  4,  9, 36, 11,  6, 10,  8, 10,  7, 30,\n",
       "         5,  9,  6, 14,  8, 10,  7, 30,  5,  9,  6, 10,  8, 10,  7, 30,  5,  9,\n",
       "         6, 10,  8, 10,  7, 30,  4, 63, 61, 33,  5, 62,  4, 15,  4, 17,  4, 16,\n",
       "         4, 21, 20, 22,  4, 19, 12, 11,  4, 18, 12, 11,  4,  9,  6, 10,  8, 10,\n",
       "         7, 29,  5,  9,  6, 10,  8, 10,  7, 30,  5,  9,  6, 10, 88, 87,  8, 10,\n",
       "         7, 29,  5, 35,  6, 10,  5, 15,  4, 17,  4, 16,  4, 43,  4, 41,  4, 39,\n",
       "         4, 38,  4, 40,  4,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_dataset.__getitem__(102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c75798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8510482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=LM_MODEL_SAVEDIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10000,\n",
    "    logging_steps=3000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=3000,\n",
    "    save_total_limit=1,\n",
    "    prediction_loss_only=False,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e0efaf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1574903/127849066.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "ret = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f70a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(LM_MODEL_SAVEDIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

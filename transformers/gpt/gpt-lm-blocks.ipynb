{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cebebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2TokenizerFast, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b724a5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmacosta\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "186a8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_SAVEDIR = Path('/home/macosta/ttmp/primus-data/blocks/blocks-v3-tokenizer/')\n",
    "LM_MODEL_SAVEDIR = Path('/home/macosta/ttmp/primus-models/gpt2-lm-blocks-nocols/')\n",
    "Path(LM_MODEL_SAVEDIR).mkdir(exist_ok=True)\n",
    "TXT_FILES = Path('/home/ibukey/ttmp/blocks-txt-v3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2343163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.cuda.device at 0x7f4b0084c290>, <torch.cuda.device at 0x7f4b0084c8d0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.cuda.device(i) for i in range(torch.cuda.device_count())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91d546d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available:  True\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "print('Cuda available: ', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93251a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab876509",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file /home/macosta/ttmp/primus-data/blocks/blocks-v3-tokenizer/config.json not found\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 30000\n",
    "MAX_LEN = 512\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(TOKENIZER_SAVEDIR, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0f5d68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000000000000111011100000111111011111111111111011111111111111011111111111111011111111111111011111111111111011111111111111011111111111111',\n",
       "  430),\n",
       " ('000000001111111000000000111111000000000111111000000000111111000000000011111000000000111111000000000111111000000000111111000000001111111000000001111111000000011111111000000111111111000001111111111000011111111111001111111111111',\n",
       "  12901),\n",
       " ('111111111111111111111111111100111111111111000111111111111000111111111110000111111111110000111111111100000111111111100000111111111100000111111111100000111111111100000111111111100000111111111100000111111111110000111111111111000',\n",
       "  5581),\n",
       " ('000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000111111111111000111111111111111111111111111111111111111111111000000001111111000000000000000000000000000000000000000000000000000000000000',\n",
       "  21154),\n",
       " ('100000000000011100000000000001100000000000000100110000000000100111111000000100111111110000100111111111100100111111111110100111111111111100111111111111100111111111111100111111111111100111111111111100111111111111100111111111111',\n",
       "  8428)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f134e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tokenizer.vocab:\n",
    "    if item[0] != '<':\n",
    "        assert(len(item) == 15 * 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71e6bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.unk_token = '<unk>'\n",
    "tokenizer.bos_token = '<s>'\n",
    "tokenizer.eos_token = '</s>'\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6992d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTUAL_VOCAB_SIZE = len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51440fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30001"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACTUAL_VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "97806405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters: 108489984\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=ACTUAL_VOCAB_SIZE,\n",
    "    n_positions=MAX_LEN,\n",
    "    n_head=12,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config=config)\n",
    "print('Num parameters:', model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f886ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, src_files, tokenizer, max_length):\n",
    "        self.examples = []\n",
    "        for src_file in tqdm(src_files):\n",
    "            words = src_file.read_text(encoding=\"utf-8\")\n",
    "            words = words.split()\n",
    "            words = ['<s>'] + words + ['</s>']\n",
    "            for i in range(0, len(words), max_length):\n",
    "                word_string = ' '.join(words[i:i+max_length])\n",
    "                tokenized = tokenizer.encode(word_string, max_length=max_length, padding='max_length')\n",
    "                assert(len(tokenized) == max_length)\n",
    "                self.examples.append(tokenized)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f9bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_datasets(tokenizer, max_length, fraction=1.0, test_size=0.1):\n",
    "    src_files = list(Path(TXT_FILES).glob(\"**/*.txt\"))\n",
    "    src_files = src_files[:int(len(src_files) * fraction)]\n",
    "    split_index = int(len(src_files) * (1 - test_size))\n",
    "    train_files = src_files[:split_index]\n",
    "    test_files = src_files[split_index:]\n",
    "    train_dataset = CustomDataset(train_files, tokenizer, max_length=max_length)\n",
    "    test_dataset = CustomDataset(test_files, tokenizer, max_length=max_length)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e75d711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 831/831 [00:09<00:00, 86.56it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:00<00:00, 86.74it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = create_train_test_datasets(tokenizer, MAX_LEN, fraction=.01, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79ed029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import ceil\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fb77e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_TOKEN = np.array([0 if i % 2 else 1 for i in range(225)], dtype=np.uint8).reshape((15, 15)) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5f4154b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_image(tokens):\n",
    "    tokens = [t for t in tokens if t not in [\"<s>\", \"</s>\", \"<pad>\"]]\n",
    "    img_array = np.ones((15 * 6 + 5 * 3, ceil(len(tokens) / 6)*15), dtype=np.uint8) * 255\n",
    "    for i in range(5):\n",
    "        img_array[i*18+15:(i*18)+3+15, :] = 0\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == '<unk>':\n",
    "            token_np = UNK_TOKEN\n",
    "        else:\n",
    "            token_np = np.array([int(x) for x in token]).reshape((15, 15))\n",
    "            token_np = token_np * 255\n",
    "        row = i % 6\n",
    "        column = i // 6\n",
    "        start_row = row * 18\n",
    "        start_column = column * 15\n",
    "        img_array[start_row:start_row+15, start_column:start_column+15] = token_np\n",
    "        img = Image.fromarray(img_array)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ad928da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     4,     4,    49,    57,   121,     4,    93,    89,    88,\n",
       "           58,   172,    74,    73,    65,    66,   171,   129,    63,    75,\n",
       "           37,     4,    59,    64,     4,     4,   191,     6,   184,     4,\n",
       "            4,     4,     4,   292,   190,     4,     4,    11,   516,   380,\n",
       "            4,     4,     4,     4,   398,     4,     4,     4,     4,     4,\n",
       "            4,  2453,  3290,     4,     4,     4,     4,  3002,  2452,     4,\n",
       "            4,     4,     4,  2267,  1650,     4,     4,     4,     4,     4,\n",
       "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
       "            4,     4,     4,   877,     4,     4,     4,     4,     4,  1758,\n",
       "            4,     4, 10411,    11,    11,  4956,     4,     4,   392, 17272,\n",
       "        16597,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
       "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
       "            4,     4,    24,    24,    24,    24,     4,     4,     4,   162,\n",
       "          566,    21,    21,     4,     4,  2208,  3018,     4,     4,     4,\n",
       "            4,    85,     4,     4,     4,     4,     4,     4,     4,     4,\n",
       "            4,     4,     4,     4,     4,     4,     4,     4,   484,   656,\n",
       "           20,    20,  5622,     4,   729,  2243,     4,     4,    10,     4,\n",
       "            4,  1422,   564,    33, 17933,     4,     4,   366,  1731,     4,\n",
       "           10,     4,   242,    11,    11,    11,  5446,     4,   645,     4,\n",
       "            4,     4,    10,     4,     4,   396,    28,    28,  4821,     4,\n",
       "            4,  1719,     4,     4,     4,  1563,  2109,    26,  4238,  3747,\n",
       "            4,   854, 12179,  7766, 10841,     4,     4,  1058,     4,  2040,\n",
       "            4,     4,     4,  1563,  2109,    26,    26,   227,     4,   854,\n",
       "          233,     4,     4,     4,     4,  1058,     4,     4,     4,     4,\n",
       "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
       "            4,     4,     4,     4,  2482,    27,    27,  5317,     4,     4,\n",
       "         1160, 11721, 10167,   109,     4,     4,     4,     4,     4,     4,\n",
       "            4,     4,     6,     6,     6,     6,     4,     4,     7,     7,\n",
       "            7,     7,     4,     4,   794,   564,    33,    33,   660,     4,\n",
       "          366,  1961,     4,     4,     4,     4,     4,     4,     4,     4,\n",
       "            4,     4,     4,     4,     4,     4,     4,     4,    45,     9,\n",
       "            9,     9,     9,     4,   575,   241,     4,     4,  2712,     4,\n",
       "          135,   802,     5,     5, 21086,     4,     4,  1062,   778,     7,\n",
       "        17908,     4,     4,   348,   140,     4,  6401,     4,     4,   495,\n",
       "           34,    34, 19131,     4,     4,  2900,     4,     4,   629,     4,\n",
       "            4,     4,  1880,    24,    24,     4,     4,     4,  1990,     4,\n",
       "            4,     4,     4,   930,   746,    18,    18,     4,     4,  1290,\n",
       "          341,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
       "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
       "            4,     4,     4, 22295, 13314,     4,     4,     4,     4, 19576,\n",
       "            4,     4,     4,    38,     6,     6,     6,   797,     4,  2839,\n",
       "         5218,     4, 13876,  1699,     4,   370,  9096,  9103,  1618,     4,\n",
       "            4,     4,     4,     4,     4,     4,     4,     2,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0df7005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoded = tokenizer.decode(train_dataset.__getitem__(2)).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a7467250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=1290x105 at 0x7F4ACCC93BD0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens_to_image(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c75798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fbd07bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 8\n",
    "N_EVALS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d4b513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEPS = len(train_dataset) * N_EPOCHS // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5229151",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EVAL = N_STEPS // N_EVALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8510482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=LM_MODEL_SAVEDIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    logging_steps=STEPS_PER_EVAL,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=STEPS_PER_EVAL,\n",
    "    save_total_limit=1,\n",
    "    prediction_loss_only=False,\n",
    "    report_to=\"wandb\",\n",
    "    save_steps=5000\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e0efaf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 2531\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 400\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmacosta\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/macosta/huggingface/runs/ec5p85ie\" target=\"_blank\">/home/macosta/ttmp/primus-models/gpt2-lm-columnwise</a></strong> to <a href=\"https://wandb.ai/macosta/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1440, in forward\n    return_dict=return_dict,\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 897, in forward\n    output_attentions=output_attentions,\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 401, in forward\n    output_attentions=output_attentions,\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 336, in forward\n    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 196, in _attn\n    attn_weights = attn_weights / (float(value.size(-1)) ** 0.5)\nRuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 23.70 GiB total capacity; 11.96 GiB already allocated; 230.56 MiB free; 12.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1728408/127849066.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m                 if (\n",
      "\u001b[0;32m~/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1972\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1973\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1440, in forward\n    return_dict=return_dict,\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 897, in forward\n    output_attentions=output_attentions,\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 401, in forward\n    output_attentions=output_attentions,\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 336, in forward\n    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n  File \"/home/macosta/anaconda3/envs/mir2/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 196, in _attn\n    attn_weights = attn_weights / (float(value.size(-1)) ** 0.5)\nRuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 23.70 GiB total capacity; 11.96 GiB already allocated; 230.56 MiB free; 12.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "ret = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f70a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(LM_MODEL_SAVEDIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

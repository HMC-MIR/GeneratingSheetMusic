{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d999fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Config\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15da417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30000\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d39b6fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_SAVEDIR = Path('/home/macosta/ttmp/primus-data/primus-semantic/semantic-tokenizer/')\n",
    "LM_MODEL_SAVEDIR = Path('/home/macosta/ttmp/primus-models/gpt2-lm-semantic-norhythm/')\n",
    "PRIMUS_TXT_FILES = Path('/home/macosta/ttmp/primus-data/primus-semantic/semantic-cleaned/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d52484a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_LEN,\n",
    "    n_head=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0e85caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config=config).from_pretrained(str(LM_MODEL_SAVEDIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be9c9806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file /home/macosta/ttmp/primus-data/primus-semantic/semantic-tokenizer/config.json not found\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(TOKENIZER_SAVEDIR, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cad7376",
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_FILES = [PRIMUS_TXT_FILES / file for file in os.listdir(PRIMUS_TXT_FILES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82590472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clef-G2 keySignature-GM timeSignature-3/4 note D5 eighth dot note E5 sixteenth barline note E5 quarter dot note D5 eighth note D5 eighth dot note E5 sixteenth barline note E5 quarter note D5 quarter note D5 eighth note D5 eighth barline note G5 quarter dot note B4 eighth note F#5 eighth dot note E5 sixteenth barline note D5 quarter note C#5 quarter \n"
     ]
    }
   ],
   "source": [
    "print(open(TXT_FILES[10], \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "208d7bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_notes(text):\n",
    "    text = re.sub(r'note (.*?) ', r'note-\\1_', text)\n",
    "    text = re.sub(r'rest ', r'rest-', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37c27e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_semantic_to_semantic(text):\n",
    "    text = re.sub(r'note (.*?) ', r'note-\\1_', text)\n",
    "    text = re.sub(r'rest ', r'rest-', text)\n",
    "    text = re.sub(' dotdot', '..', text)\n",
    "    text = re.sub(' dot', '.', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d6b24e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rhythm_util as ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "286e9479",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESIGS = [\n",
    "    'timeSignature-3/8',\n",
    "    'timeSignature-6/8',\n",
    "    'timeSignature-2/4',\n",
    "    'timeSignature-3/4', \n",
    "    'timeSignature-C', \n",
    "    'timeSignature-5/4', \n",
    "    'timeSignature-6/4',\n",
    "    'timeSignature-7/4',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77e95f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYSIGS = [\n",
    "    'keySignature-FM',\n",
    "    'keySignature-DbM',\n",
    "    'keySignature-GM',\n",
    "    'keySignature-EM',\n",
    "    'keySignature-DM',\n",
    "    'keySignature-C#M',\n",
    "    'keySignature-AM',\n",
    "    'keySignature-AbM',\n",
    "    'keySignature-F#M',\n",
    "    'keySignature-CM',\n",
    "    'keySignature-GbM',\n",
    "    'keySignature-BM',\n",
    "    'keySignature-BbM',\n",
    "    'keySignature-EbM'\n",
    "]\n",
    "\n",
    "CLEFS = [\n",
    "    'clef-G2',\n",
    "    'clef-C5',\n",
    "    'clef-C2',\n",
    "    'clef-F4',\n",
    "    'clef-C1',\n",
    "    'clef-C4',\n",
    "    'clef-C3',\n",
    "    'clef-F3',\n",
    "    'clef-F5',\n",
    "    'clef-G1'\n",
    "]\n",
    "\n",
    "def generate_randomized_semantic_seed(timesig, start_token=\"<s>\"):\n",
    "    keysig = KEYSIGS[int(random.random() * len(KEYSIGS))]\n",
    "    clef = CLEFS[int(random.random() * len(CLEFS))]\n",
    "    return ' '.join([start_token, clef, keysig, timesig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a768bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c4b58ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_timesig_generated(n, timesig, savedir):\n",
    "    savedir.mkdir(exist_ok=True, parents=True)\n",
    "    input_ids = tokenizer.encode(generate_randomized_semantic_seed(timesig), return_tensors='pt')\n",
    "    for i in tqdm(range(n)):\n",
    "        output_tokens = model.generate(input_ids, \n",
    "                                       pad_token_id=1,\n",
    "                                       eos_token_id=2,\n",
    "                                       temperature=1,\n",
    "                                       max_length=128,\n",
    "                                       do_sample=True)[0]\n",
    "        output_tokens = tokenizer.decode(output_tokens).split()\n",
    "        output = join_notes(' '.join(output_tokens))\n",
    "        output = re.sub(' dotdot', '..', output)\n",
    "        output = re.sub(' dot', '.', output)\n",
    "        if output[-5:] == ' </s>':\n",
    "            output = output[:-5]\n",
    "        if output[:4] == '<s> ':\n",
    "            output = output[4:]\n",
    "        with open(savedir / f\"generated_{i}.txt\", \"w\") as f:\n",
    "            f.write(output)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2309531f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:13<00:00,  1.35s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:13<00:00,  1.31s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.17s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.17s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:09<00:00,  1.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:12<00:00,  1.27s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:09<00:00,  1.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:12<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "N = 2000\n",
    "for t in TIMESIGS:\n",
    "    save_timesig_generated(N, t, Path(f'/home/macosta/ttmp/generated-semantic-final/{t.replace(\"/\",\"-\")}/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85501c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 5000\n",
    "# for t in TIMESIGS:\n",
    "#     correct = check_correctness(N, t, log_frequency=100)\n",
    "#     print(f'\\nNUM CORRECT FOR {t}: {correct} / {N}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ad4da3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import semantic_to_pae as stp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f918984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_incipits(n, directory):\n",
    "    input_str = tokenizer.encode(\"<s>\", return_tensors=\"pt\")\n",
    "    for i in tqdm(range(n)):\n",
    "        output_tokens = model.generate(input_str, \n",
    "                                       pad_token_id=1,\n",
    "                                       eos_token_id=2,\n",
    "                                       temperature=1,\n",
    "                                       max_length=128,\n",
    "                                       do_sample=True)[0]\n",
    "        output_tokens = tokenizer.decode(output_tokens).split()\n",
    "        output = join_notes(' '.join(output_tokens))\n",
    "        output = re.sub(' dotdot', '..', output)\n",
    "        output = re.sub(' dot', '.', output)\n",
    "        if output[-5:] == ' </s>':\n",
    "            output = output[:-5]\n",
    "        if output[:4] == '<s> ':\n",
    "            output = output[4:]\n",
    "        stp.convert_and_save(output, Path(directory) / f\"output-{i}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "638a6cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 30/30 [00:57<00:00,  1.93s/it]\n"
     ]
    }
   ],
   "source": [
    "# create_incipits(30, '/home/macosta/ttmp/generated-semantic-nonrhythmic/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82ba06b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_str = tokenizer.encode('clef-G2 keySignature-GM timeSignature-3/4 note D5 eighth dot note E5 sixteenth barline note E5 quarter dot note D5 eighth note D5 eighth dot note E5 sixteenth barline note E5 quarter note D5 quarter note D5 eighth note D5 eighth barline note G5 quarter dot note B4 eighth note F#5 eighth dot note E5 sixteenth barline note D5 quarter note C#5 quarter', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "befe42bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21, 45, 40,  4,  9,  5, 14,  4, 16,  8,  7,  4, 16,  6, 14,  4,  9,  5,\n",
       "          4,  9,  5, 14,  4, 16,  8,  7,  4, 16,  6,  4,  9,  6,  4,  9,  5,  4,\n",
       "          9,  5,  7,  4, 19,  6, 14,  4, 17,  5,  4, 28,  5, 14,  4, 16,  8,  7,\n",
       "          4,  9,  6,  4, 26,  6]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2ecc90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_tokens = model.generate(input_str, \n",
    "#                                pad_token_id=1,\n",
    "#                                eos_token_id=2,\n",
    "#                                temperature=1,\n",
    "#                                max_length=128,\n",
    "#                                do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a929c2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21, 45, 40,  4,  9,  5, 14,  4, 16,  8,  7,  4, 16,  6, 14,  4,  9,  5,\n",
       "          4,  9,  5, 14,  4, 16,  8,  7,  4, 16,  6,  4,  9,  6,  4,  9,  5,  4,\n",
       "          9,  5,  7,  4, 19,  6, 14,  4, 17,  5,  4, 28,  5, 14,  4, 16,  8,  7,\n",
       "          4,  9,  6,  4, 26,  6,  2]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30f808b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clef-G2 keySignature-GM timeSignature-3/4 note D5 eighth dot note E5 sixteenth barline note E5 quarter dot note D5 eighth note D5 eighth dot note E5 sixteenth barline note E5 quarter note D5 quarter note D5 eighth note D5 eighth barline note G5 quarter dot note B4 eighth note F#5 eighth dot note E5 sixteenth barline note D5 quarter note C#5 quarter </s>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.decode(output_tokens[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

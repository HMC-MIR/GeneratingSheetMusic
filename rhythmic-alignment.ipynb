{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fded432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "674c83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIMUS_LOCATION = Path('/home/macosta/ttmp/primus-data/primus/')\n",
    "PMA_PATH = Path('/home/macosta/ttmp/primus-data/primus-mei-agnostic-png/')\n",
    "CROPPED_PATH = Path('/home/macosta/ttmp/primus-data/cropped/cropped-txt/')\n",
    "PMA_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54dff0ad",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██████▌                                                                                            | 2922/44084 [00:56<13:12, 51.97it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3322465/192170366.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mincipit_savepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAVEPATH\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mincipit_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mincipit_savepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmei_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincipit_savepath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmei_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magnostic_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincipit_savepath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0magnostic_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpng_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincipit_savepath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpng_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mir2/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                 \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mir2/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m\"\"\"copy data from file-like object fsrc to file-like object fdst\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for package in os.listdir(PRIMUS_LOCATION):\n",
    "    for incipit_dir in tqdm(os.listdir(PRIMUS_LOCATION / package)):\n",
    "        incipit_patho = PRIMUS_LOCATION / package / incipit_dir\n",
    "        files = os.listdir(incipit_path)\n",
    "        mei_file = incipit_path / [f for f in files if f[0] != '.' and f[-4:] == '.mei'][0]\n",
    "        agnostic_file = incipit_path / [f for f in files if f[0] != '.' and f[-9:] == '.agnostic'][0]\n",
    "        png_file = ncipit_path / [f for f in files if f[0] != '.' and f[-4:] == '.png'][0]\n",
    "        incipit_savepath = PMA_PATH / incipit_dir\n",
    "        incipit_savepath.mkdir(exist_ok=True)\n",
    "        shutil.copyfile(mei_file, incipit_savepath / mei_file.name)\n",
    "        shutil.copyfile(agnostic_file, incipit_savepath / agnostic_file.name)\n",
    "        shutil.copyfile(png_file, incipit_savepath / png_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d47a65ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 87679/87679 [02:54<00:00, 503.56it/s]\n"
     ]
    }
   ],
   "source": [
    "for incipit_txt in tqdm(os.listdir(CROPPED_PATH)):\n",
    "    from_path = CROPPED_PATH / incipit_txt\n",
    "    if not os.path.isfile(from_path):\n",
    "        continue\n",
    "    to_path = PMA_PATH / Path(incipit_txt[:-4]) / incipit_txt\n",
    "    shutil.copyfile(from_path, to_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec2eb5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StafflineTokenizer():\n",
    "    def __init__(self, stafflines, ignore, n_rep=1):\n",
    "        stafflines_w_spaces = [f\"{x} \" for x in stafflines]\n",
    "        ignore_w_spaces = [f\"{x} \" for x in ignore]\n",
    "        self.staff_pattern = re.compile(f\"({'|'.join(stafflines_w_spaces)})\" + \"{\" + str(n_rep) + \",}\")\n",
    "        self.ignore_pattern = re.compile(f\"({'|'.join(ignore_w_spaces)})+\")\n",
    "    def __call__(self, items):\n",
    "        return (self.tokenize_item(x) for x in items)\n",
    "    def tokenize_item(self, item):\n",
    "        if item[-1] != ' ':\n",
    "            item += ' '\n",
    "        item = self.ignore_pattern.sub(\"\", item)\n",
    "        staffline_stripped = self.staff_pattern.sub(\"& \", item)\n",
    "        tokens = staffline_stripped.split(\"& \")\n",
    "        tokens = [t.strip().replace(' ', '_') for t in tokens if t]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33ae36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAFFLINE_A = \"0000000000000000000000000000000000000000000000000001000000000000000001000000000000000001000000000000000001000000000000000001000000000000000000000000000000000000000000000000000\"\n",
    "STAFFLINE_B = \"0000000000000000000000000000000000000000000000000011100000000000000011100000000000000011100000000000000011100000000000000011100000000000000000000000000000000000000000000000000\"\n",
    "IGNORE = \"0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\"\n",
    "staffline_tokenizer = StafflineTokenizer(stafflines=[STAFFLINE_A, STAFFLINE_B], n_rep=3, ignore=[IGNORE])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "682acf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_token(token):\n",
    "    cols = token.split('_')\n",
    "    arr = np.zeros((len(cols), 175), dtype=np.uint8)\n",
    "    for i, col in enumerate(cols):\n",
    "        arr[i] = [(1-int(x)) for x in col]\n",
    "    Image.fromarray(arr.T * 255).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5ac2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEF = 'clef'\n",
    "C_CLEF = 'C'\n",
    "G_CLEF = 'G'\n",
    "F_CLEF = 'F'\n",
    "ACCIDENTAL = 'accidental'\n",
    "NOTE = 'note'\n",
    "DIGIT = 'digit'\n",
    "BARLINE = 'barline'\n",
    "DOT = 'dot'\n",
    "BEAMED = 'beamed'\n",
    "BEAM_LEFT = 'beamedLeft'\n",
    "BEAM_RIGHT = 'beamedRight'\n",
    "BEAM_BOTH = 'beamedBoth'\n",
    "SLUR = 'slur'\n",
    "START = 'start'\n",
    "WHOLE = 'whole'\n",
    "SPACE = 'S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50bebf7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▍                                   | 5556/87678 [02:24<36:28, 37.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3698/5000 73.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|████▋                                | 11101/87678 [04:48<31:11, 40.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7467/10000 74.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████                              | 16617/87678 [07:13<32:35, 36.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11153/15000 74.35333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▎                           | 22164/87678 [09:38<24:10, 45.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14853/20000 74.265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████▋                         | 27727/87678 [12:03<26:44, 37.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18595/25000 74.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████                       | 33294/87678 [14:27<25:35, 35.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22286/30000 74.28666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████▍                    | 38832/87678 [16:52<24:16, 33.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25954/35000 74.15428571428572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|██████████████████▋                  | 44385/87678 [19:17<20:01, 36.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29692/40000 74.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████                | 49927/87678 [21:41<16:35, 37.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33392/45000 74.20444444444445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|███████████████████████▍             | 55491/87678 [24:06<15:39, 34.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37071/50000 74.142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████▊           | 61051/87678 [26:31<11:04, 40.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40763/55000 74.11454545454545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|████████████████████████████         | 66640/87678 [28:56<09:04, 38.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44458/60000 74.09666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|██████████████████████████████▍      | 72200/87678 [31:21<07:11, 35.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48102/65000 74.00307692307692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████▊    | 77790/87678 [33:46<03:53, 42.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51761/70000 73.94428571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|███████████████████████████████████▏ | 83325/87678 [36:11<01:49, 39.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55412/75000 73.88266666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 87678/87678 [38:03<00:00, 38.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58338/78916 73.92417253788838\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "correct = 0\n",
    "attempted = 0\n",
    "for incipit_dir in tqdm(os.listdir(PMA_PATH)):\n",
    "#     if incipit_dir != '000105782-1_1_1':\n",
    "#         continue\n",
    "    incipit_path = PMA_PATH / incipit_dir\n",
    "    agnostic_file = incipit_path / f\"{incipit_dir}.agnostic\"\n",
    "    png_file = incipit_path / f\"{incipit_dir}.png\"\n",
    "#     Image.open(png_file).show()\n",
    "    txt_file = incipit_path / f\"{incipit_dir}.txt\"\n",
    "    with open(agnostic_file, \"r\") as f:\n",
    "        agnostic_contents = f.read()\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        txt_contents = f.read()\n",
    "    agnostic_contents = re.sub(' +', ' ', agnostic_contents)\n",
    "    agnostic_symbols = agnostic_contents.strip().split()\n",
    "    if SLUR in [symbol[:4] for symbol in agnostic_symbols]:\n",
    "        continue\n",
    "    tokens = staffline_tokenizer.tokenize_item(txt_contents)\n",
    "#     for token in tokens:\n",
    "#         show_token(token)\n",
    "    rhythmic_tokens = []\n",
    "    token_index = 0\n",
    "    agnostic_index = 0\n",
    "    try:\n",
    "        while agnostic_index < len(agnostic_symbols) and token_index < len(tokens):\n",
    "            agnostic_symbol = agnostic_symbols[agnostic_index]\n",
    "            token = tokens[token_index]\n",
    "            split_on_hyphen = agnostic_symbol.split('-')\n",
    "            symbol_type, location = split_on_hyphen[0], ''.join(split_on_hyphen[1:])\n",
    "            if '.' in symbol_type:\n",
    "                symbol_type, details = symbol_type.split('.')\n",
    "                if symbol_type == CLEF:\n",
    "                    # If C clef, combine two tokens because of whitespace in between\n",
    "                    if details == C_CLEF or details == F_CLEF:\n",
    "                        clef_token = '_'.join(tokens[token_index:token_index+2])\n",
    "                        token_index += 1\n",
    "                    # Other clefs are a single token\n",
    "                    else:\n",
    "                        clef_token = token\n",
    "                    rhythmic_tokens.append((clef_token, agnostic_symbol))\n",
    "                elif symbol_type == DIGIT:   \n",
    "                    # Digits are only their own token in the time signature; combine two agnostic symbols\n",
    "                    if agnostic_index < len(agnostic_symbols) - 1 \\\n",
    "                    and agnostic_symbols[agnostic_index + 1].split('.')[0] == DIGIT and \\\n",
    "                    agnostic_symbols[agnostic_index + 1].split('-')[1] != location:\n",
    "                        time_sig = ' '.join(agnostic_symbols[agnostic_index:agnostic_index+2])\n",
    "                        agnostic_index += 1\n",
    "                        rhythmic_tokens.append((token, time_sig))\n",
    "                    else:\n",
    "                        token_index -= 1\n",
    "                elif symbol_type == NOTE:\n",
    "                    if len(details) > len(BEAMED) and details[:len(BEAMED)] == BEAMED:\n",
    "                        a_symbols = []\n",
    "                        while agnostic_index < len(agnostic_symbols):\n",
    "                            agnostic_symbol_ = agnostic_symbols[agnostic_index]\n",
    "                            split_on_hyphen = agnostic_symbol_.split('-')\n",
    "                            symbol_type_, location_ = split_on_hyphen[0], ''.join(split_on_hyphen[1:])\n",
    "                            if '.' not in symbol_type_:\n",
    "                                a_symbols.append(agnostic_symbol_)\n",
    "                                agnostic_index += 1\n",
    "                                continue\n",
    "                            symbol_type_, details_ = symbol_type_.split('.')\n",
    "                            if len(details_) > len(BEAM_LEFT) and \\\n",
    "                            details_[:-1] == BEAM_LEFT:\n",
    "                                a_symbols.append(agnostic_symbol_)\n",
    "                                break\n",
    "                            a_symbols.append(agnostic_symbol_)\n",
    "                            agnostic_index += 1\n",
    "                        rhythmic_tokens.append((token, ' '.join(a_symbols)))\n",
    "#                     elif details == WHOLE and location[0] == SPACE and 1 <= int(location[1]) <= 4:\n",
    "#                         whole_note = '_'.join(tokens[token_index:token_index+2])\n",
    "#                         token_index += 1\n",
    "#                         rhythmic_tokens.append((whole_note, agnostic_symbol))\n",
    "                    else:\n",
    "                        rhythmic_tokens.append((token, agnostic_symbol))\n",
    "                else:\n",
    "                    rhythmic_tokens.append((token, agnostic_symbol))\n",
    "            else:\n",
    "                rhythmic_tokens.append((token, agnostic_symbol))\n",
    "            token_index += 1\n",
    "            agnostic_index += 1\n",
    "        if token_index == len(tokens) and agnostic_index == len(agnostic_symbols):\n",
    "            correct += 1\n",
    "#         else:\n",
    "#             print(png_file)\n",
    "#             print(agnostic_symbols)\n",
    "#             Image.open(png_file).show()\n",
    "#             for t, symbol in rhythmic_tokens:\n",
    "#                 show_token(t)\n",
    "#                 print(symbol)\n",
    "        attempted += 1\n",
    "        i += 1\n",
    "        if i % 5000 == 0:\n",
    "            print(f\"{correct}/{attempted} {correct * 100 / attempted}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "print(f\"{correct}/{attempted} {correct * 100 / attempted}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
